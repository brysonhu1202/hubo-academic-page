---
title: Improving your statistical inferences
subtitle: Improving your statistical inferencesï¼ŒDaniel Lakens
authors:
Â  - admin
tags: [æ•°æ®ç§‘å­¦]
categories: [Data Analysis]
date: '2023-12-17T00:00:00Z'
lastMod: '2023-07-29T00:00:00Z'
image:
Â  caption: ''
Â  focal_point: ''
---
# Introduction + Frequentist Statistics

Statistical inference is a process where you use data from a sample to describe properties of the distribution of data in the population. When you test a hypothesis, calculate a confidence interval, or estimate an effect size, you are making statistical inferences.

ä¸‰ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•
1. Neyman-Pearson:é€šè¿‡æ˜¾è‘—æ€§æ°´å¹³æ¥å—è™šæ— å‡è®¾è¿˜æ˜¯å¯¹ç«‹å‡è®¾
2. Bayesian Statistics: è®¡ç®—å¯¹å‡è®¾çš„ä¿¡å¿µ
3. Likelihoodï¼šæ”¯æŒä¸åŒå‡è®¾çš„ä¼¼ç„¶æ€§

## p value
på€¼æ˜¯åœ¨è™šæ— å‡è®¾ä¸ºçœŸçš„å‰æä¸‹ï¼Œè§‚å¯Ÿå€¼å±äºæ¬¡åˆ†é…çš„æ¦‚ç‡
The formal definition of a p-value is the probability of getting the observed,Â or more extreme data, assuming the null hypothesis is true.

è§£é‡Špå€¼

å½“ä¸¤ç»„çš„å‡å€¼å·®å¼‚ä¸º0.11æ—¶ï¼Œå¦‚ä½•è§£é‡Šï¼Ÿ
- å¯èƒ½æ˜¯éšæœºçš„å™ªéŸ³
- å¯èƒ½æ˜¯çœŸå®çš„å·®å¼‚

From the data that we have, we can calculate means, standard deviations, andÂ we know the sample size that we have.Â We can use these parameters to calculate a test statistic, andÂ compare this test statistic against a distribution.
é€šè¿‡å‡å€¼ã€æ ‡å‡†å·®ä»¥åŠæ ·æœ¬é‡æ¥è®¡ç®—åˆ†å¸ƒå·®å¼‚ï¼ˆé€šå¸¸ä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰

å¦‚æœé›¶å‡è®¾ä¸ºçœŸï¼Œå°±æœ‰95%çš„èµ„æ–™å±äºè“è‰²åŒºåŸŸï¼Œå¦‚æœèµ„æ–™ç®—å‡ºçš„ç»Ÿè®¡å€¼åœ¨ä¸¤ä¸ªä¸´ç•Œå€¼ä¹‹å†…ï¼Œå°±æ²¡æœ‰æ•ˆæœ
![[Improving your statistical inferences_image_1.png]]
å¦‚æœèµ„æ–™è½å…¥é‡è¡¨ï¼Œä»£è¡¨æœ‰æ•ˆæœ
![[Improving your statistical inferences_image_2.png]]

på€¼æ˜¯ä½ è§‚å¯Ÿåˆ°çš„æ•°æ®çš„æ¦‚ç‡ï¼Œä¸æ˜¯ä¸€ä¸ªç†è®ºçš„æ¦‚ç‡

på€¼ä¸æ˜¯æ”¯æŒè™šæ— å‡è®¾ä¸ºçœŸçš„çœŸå®æ¦‚ç‡
$P\left(D^* \mid H\right) \neq P(H \mid D)$ï¼ˆD= data, H = hypothesisï¼‰
å·¦è¾¹ï¼šthe probabilityÂ of the data assuming the null hypothesis is true.Â å‡è®¾é›¶å‡è®¾ä¸ºçœŸçš„æ•°æ®æ¦‚ç‡ã€‚
å³è¾¹ï¼šthe probability of an hypothesisÂ given some data that you have observed.Â åœ¨è§‚å¯Ÿåˆ°ä¸€äº›æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå‡è®¾æˆç«‹çš„æ¦‚ç‡ã€‚

åŒæ—¶ï¼Œp>.05ä¸ä»£è¡¨æ²¡æœ‰æ•ˆæœï¼Œä½ å¯èƒ½éœ€è¦æ›´å¤§çš„æ ·æœ¬å»æ£€æµ‹å°æ•ˆåº”ï¼Œæ˜¯è¿™ä¸ªé—®é¢˜è¿˜æ²¡æœ‰ç»“æœ

Using p-values correctly:
p < alpha: Act as if data is not noise.  
p > alpha: Remain uncertain or act as if data is noise.

é”™è¯¯çš„è§£é‡Špå€¼ï¼šWe found a p-value smaller than 0.05, so our theory...
æ­£ç¡®çš„è§£é‡Špå€¼ï¼šWe found the p-value smaller than 0.05, so our data...


è´å¶æ–¯æ˜¯èƒ½å¤Ÿè®¡ç®—ç†è®ºä¸ºçœŸçš„è®°å½•çš„æ–¹æ³•ï¼Œ

## Iç±»é”™è¯¯ä¸IIç±»é”™è¯¯

Iç±»é”™è¯¯ï¼šä½ ä»¥ä¸ºæ˜¯å…¶å®ä¸æ˜¯ï¼Œè¯¯è®¤
IIç±»é”™è¯¯ï¼šä½ ä»¥ä¸ºä¸æ˜¯å…¶å®æ˜¯ï¼Œæ¼è®¤

H0ï¼šè™šæ— å‡è®¾ H1ï¼šå¤‡æ‹©å‡è®¾

Î±æ˜¯æŒ‡æˆ‘ä»¬æ¥å—H0ä¸ºçœŸæ—¶å€™ç»“æœæ˜¾è‘—çš„çŠ¯é”™å‡ ç‡ï¼ˆIç±»é”™è¯¯ã€‚ä¸åº”è¯¥æ˜¾è‘—ï¼‰ï¼Œå¸¸è§è®¾å®šä¸º5%
Î²æ˜¯æŒ‡æˆ‘ä»¬æ¥å—H1ä¸ºçœŸæ—¶å€™ç»“æœä¸æ˜¾è‘—çš„çš„çŠ¯é”™å‡ ç‡ï¼ˆIIç±»é”™è¯¯ã€‚åº”è¯¥æ˜¾è‘—ï¼‰ï¼Œ
1-Î²ä¾¿æŒ‡ç»“æœæ˜¾è‘—åŒæ—¶H1ä¸ºçœŸçš„å¯èƒ½æ€§ï¼ˆç»Ÿè®¡æ£€éªŒåŠ›ï¼‰ã€‚

![[Improving your statistical inferences_image_3.png]]


å‡è®¾ç ”ç©¶ï¼šH0ä¸H1çš„æ¦‚ç‡éƒ½ä¸º50%ï¼ŒÎ±=5%ï¼Œ1-Î²=80%ã€‚
çœŸé˜´æ€§çš„å¯èƒ½æ€§æœ€å¤§

![[Improving your statistical inferences_image_4.png]]

æé«˜ç»Ÿè®¡æ£€éªŒåŠ›æ¥å¢åŠ çœŸé˜³æ€§æ¦‚ç‡ï¼šH0ä¸H1çš„æ¦‚ç‡éƒ½ä¸º50%ï¼ŒÎ±=5%ï¼Œ1-Î²=99%ã€‚
çœŸé˜³æ€§çš„å¯èƒ½æ€§æœ€å¤§
![[Improving your statistical inferences_image_5.png]]

å¦ä¸€ä¸ªæ€è·¯ï¼Œæ”¹å˜å‡è®¾å…ˆéªŒï¼šH0ä¸H1çš„æ¦‚ç‡ä¸º10%ä¸90%ï¼ŒÎ±=5%ï¼Œ1-Î²=80%ã€‚
çœŸé˜³æ€§çš„å¯èƒ½æ€§æœ€å¤§ã€‚
![[Improving your statistical inferences_image_6.png]]

ç»“æœçš„å¯è§†åŒ–
![[Improving your statistical inferences_image_7.png]]

# Likelihoods & Bayesian Statistics

## Likelihoods

Likelihoods are a way to express the relative evidence forÂ one hypothesis over another hypothesis.

äºŒé¡¹å¼æœ€å¤§ä¼¼ç„¶æ¯”å‡½æ•°ï¼š

$L(\theta)=\frac{n !}{x !(n-x) !} \times \theta^x \times(1-\theta)^{n-x}$

æŠ•å…«æ¬¡ç¡¬å¸åæ¬¡æœä¸Šï¼Œ$\theta$ =0.8 çš„ L=0.30ï¼Œé€šè¿‡å‡½æ•°å¯ä»¥å‘ç°$\theta$ =0.8ä¼¼ç„¶æ¯”æœ€å¤§

![[Improving your statistical inferences_image_8.png]]

likehood ratio ä¼¼ç„¶æ¯”ï¼šè™šæ— å‡è®¾æˆç«‹å¯¹æ¯”å¤‡æ‹©å‡è®¾æˆç«‹çš„æ¯”ä¾‹ï¼ˆç›¸å¯¹è¯æ®å¼ºåº¦ï¼‰ã€‚æœ€å¤§ä¼¼æ¯”ä¸­çš„8å’Œ32ä¸ºä¸­ç­‰å¼ºåº¦ä¸å¤§å¼ºåº¦çš„ä¸´ç•Œå€¼ã€‚

ä¾‹ï¼šè™šæ— å‡è®¾ï¼ˆ$\theta$ =0.5 ï¼‰å¯¹æ¯”å¤‡æ‹©å‡è®¾ï¼ˆ$\theta$ =0.8ï¼‰ã€‚å°†0.8çš„ä¼¼ç„¶å€¼é™¤ä»¥0.5çš„ä¼¼ç„¶å€¼ï¼ˆ6.87ï¼‰ï¼Œå¾—å‡ºï¼ˆ$\theta$ =0.8ï¼‰çš„å¯èƒ½æ€§æ˜¯ï¼ˆ$\theta$ =0.5ï¼‰çš„å…­å€
![[Improving your statistical inferences_image_9.png]]
é”™è¯¯ç°è±¡ï¼Œè™½ç„¶æœ€å¤§ä¼¼ç„¶æ¯”å¾ˆå¤§ï¼Œä½†æ˜¯ä¸¤ä¸ªå‡è®¾éƒ½æ˜¯é”™è¯¯çš„

![[Improving your statistical inferences_image_10.png]]
ä¾‹ï¼šå½“ä¸‰æ¬¡å®éªŒï¼Œä¸¤æ¬¡å®éªŒæ˜¾è‘—ï¼ˆÎ±=.05ï¼‰
H0ä¸ºçœŸçš„ä¼¼ç„¶å€¼ = 0.05 x 0.05 x 0.95 = 0.0024
H1ä¸ºçœŸçš„ä¼¼ç„¶å€¼ = 0.8 x 0.8 x 0.2 = 0.128 ï¼ˆpower =0.8ï¼‰
likehood ratioï¼š0.128/0.0024 = 54 (strong)
H1 is 54 times more likely than H0
```
å›¾æ˜¯æ€ä¹ˆå½¢æˆçš„ï¼ŸæŒ‰æˆ‘çš„ç†è§£ï¼Œè¿™æ˜¯2/3çš„ä¼¼ç„¶å€¼å›¾ï¼Œæœ€å¼€å§‹çš„äº‹8/10çš„ä¼¼ç„¶å€¼å›¾
```
![[Improving your statistical inferences_image_11.png]]

ä¸åŒæƒ…å†µä¸‹çš„ä¼¼ç„¶å€¼å›¾
![[Improving your statistical inferences_image_12.png]]```
```
ç»˜åˆ¶ä¼¼ç„¶æ›²çº¿ä»£ç 
#plot likelihood curve----
n<-10 #set total trials
x<-8 #set successes
theta<- seq(0,1,len=100) #create theta variable, from 0 to 1
like <- dbinom(x,n,theta) #create likelihood function
plot(theta,like,type='l',xlab=expression(theta), ylab='Likelihood', main="Likelihood Curve")
```


```
è®¡ç®—ä¼¼ç„¶æ¯”ä»£ç 
#Calculate the likelihood ratio----
n<-10 #set total trials
x<-5 #set successes
H0 <- .5 #specify one hypothesis you want to compare with the likihood ratio
H1 <- .4 #specify another hypothesis you want to compare with the likihood ratio (you can use 1/20, or 0.05)
dbinom(x,n,H0)/dbinom(x,n,H1) #Returns the likelihood ratio of H0 over H1
dbinom(x,n,H1)/dbinom(x,n,H0) #Returns the likelihood ratio of H1 over H0

theta<- seq(0,1,len=100) #create theta variable, from 0 to 1
like <- dbinom(x,n,theta)
#png(file="LikRatio.png",width=4000,height=3000, , units = "px", res = 900)
plot(theta,like,type='l',xlab=expression(theta), ylab='Likelihood', lwd=2)
points(H0,dbinom(x,n,H0))
points(H1,dbinom(x,n,H1))
segments(H0, dbinom(x,n,H0), x/n, dbinom(x,n,H0), lty=2, lwd=2)
segments(H1, dbinom(x,n,H1), x/n, dbinom(x,n,H1), lty=2, lwd=2)
segments(x/n, dbinom(x,n,H0), x/n, dbinom(x,n,H1), lwd=2)
title(paste('Likelihood Ratio H0/H1:',round(dbinom(x,n,H0)/dbinom(x,n,H1),digits=2)," Likelihood Ratio H1/H0:",round(dbinom(x,n,H1)/dbinom(x,n,H0),digits=2)))
#dev.off()
```

## Binomial Bayesian Inference

på€¼ä»£è¡¨ä»¥è™šæ— å‡è®¾ä¸ºçœŸçš„å‰æä¸‹ï¼Œä¸è®ºæ‰‹ä¸Šèµ„æ–™æ˜¯å¦æç«¯ï¼Œå…¶ç¬¦åˆè™šæ— å‡è®¾çš„ç¨‹åº¦
$P\left(D_{(o r>D)} \mid H 0\right)$
æˆ‘ä»¬æƒ³è¦çš„æ˜¯æ ¹æ®æ‰‹ä¸Šèµ„æ–™ï¼Œè™šæ— å‡è®¾å­˜åœ¨çš„ç¨‹åº¦ï¼Œå³åéªŒæ¦‚ç‡
$\mathrm{P}(\mathrm{H0} \mid \mathrm{D})$

åéªŒæ¦‚ç‡æ˜¯æŒ‡æ ¹æ®æ‰‹ä¸Šèµ„æ–™ï¼ŒåŠ ä¸Šå·²å­˜åœ¨ç ”ç©¶è€…è„‘ä¸­çš„ä¿¡å¿µï¼Œèƒ½ä¸èƒ½æ´»åŠ¨è™šæ— å‡è®¾æˆ–å¤‡æ‹©å‡è®¾ä¸ºçœŸçš„å‡ ç‡ã€‚ä¹Ÿå°±æ˜¯å…ˆéªŒä¹˜ä»¥ä¼¼ç„¶æ¯”çš„ä¹˜ç§¯[[Improving your statistical inferences#Likelihoods|Likelihoods]]
$Prior Belief + Data = Posterior Belief$

$\frac{P(\mathrm{H} 1 \mid \mathrm{D})}{P(\mathrm{H} 0 \mid \mathrm{D})}=\frac{P(\mathrm{D} \mid \mathrm{H} 1)}{P(\mathrm{D} \mid \mathrm{H} 0)} \times \frac{P(\mathrm{H} 1)}{P(\mathrm{H} 0)}$

$Posterior = Likelihood Ratio \times Prior$

å¯¹äºå…ˆéªŒï¼Œä½¿ç”¨betaåˆ†å¸ƒï¼Œbetaåˆ†å¸ƒç”±Î±å’ŒÎ²å†³å®šï¼ˆå’ŒIç±»IIç±»é”™è¯¯ä¸­çš„ä¸ä¸€æ ·ï¼‰

$f(x ; \alpha, \beta)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}$

ä¾‹ï¼šÎ±=1å’ŒÎ²=1ï¼ŒèŒƒå›´å†…ä»»ä¸€$\theta$çš„å‡ ç‡ç›¸åŒï¼Œä»»ä½•ç»“æœéƒ½å¯èƒ½å‘ç”Ÿï¼ˆè®¾å®šè¿™ç§å…ˆéªŒç­‰äºæ²¡æœ‰å…ˆéªŒï¼‰

![[Improving your statistical inferences_image_13.png]]

![[Improving your statistical inferences_image_14.png]]
åéªŒæ˜¯ä¸€ä¸ªæ–°çš„betaåˆ†å¸ƒ

ğ›¼* = ğ›¼prior + ğ›¼likelihood â€“ 1

ğ›½* = ğ›½prior + ğ›½likelihood â€“ 1

æ•°æ®ç»“æœï¼ˆè“è‰²è™šçº¿ï¼‰
åéªŒï¼ˆé»‘çº¿ï¼‰= æ•°æ®ç»“æœï¼ˆè“è‰²è™šçº¿ï¼Œä¼¼ç„¶æ€§å‡½æ•°ï¼‰ x å…ˆéªŒï¼ˆç°çº¿ï¼‰
![[Improving your statistical inferences_image_15.png]]

The Bayes factor is the relative evidence for one model compared to another model.
è´å¶æ–¯å› å­æ˜¯æ‰‹ä¸Šè¯æ®æ”¯æŒä¸¤ç§æ¨¡å‹çš„ç›¸å¯¹ç¨‹åº¦

![[Improving your statistical inferences_image_16.png]]
![[Improving your statistical inferences_image_17.png]]
```
è®¡ç®—è´å¶æ–¯å› å­ä»£ç 
H0<-0.5 #Set the point null hypothesis you want to calculate the Bayes Factor for
n<-20 #set total trials
x<-10 #set successes
aprior<-1 #Set the alpha for the Beta distribution for the prior
bprior<-1 #Set the beta for the Beta distribution for the prior

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="PriorLikelihoodPosterior.png",width=3000,height=3000, res = 500)
prior <- dbeta(theta, aprior, bprior)
likelihood <- dbeta(theta, alikelihood, blikelihood)
posterior <- dbeta(theta, aposterior, bposterior)
plot(theta, posterior, ylim=c(0, 15), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1)
lines(theta, prior, col="grey", lwd = 3)
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue")
BF10<-dbeta(H0, aposterior, bposterior)/dbeta(H0, aprior, bprior)
points(H0,dbeta(H0, aposterior, bposterior), pch = 19)
points(H0,dbeta(H0, aprior, bprior), pch = 19, col="grey")
segments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty=2)
title(paste('Bayes Factor:',round(BF10,digits=2)))
```

## Bayesian Thinking

![[Improving your statistical inferences_image_18.png]]

ç»“åˆäº†p Valueä¸å…ˆéªŒçš„åŒçº¿å›¾

![[Improving your statistical inferences_image_19.png]]
ä¾‹ï¼šè™šæ— å‡è®¾ä¸ºçœŸçš„äº‹å‰æ¦‚ç‡ä¸º50%ï¼Œæ”¶é›†èµ„æ–™å¾—åˆ°åˆšå¥½æ˜¾è‘—çš„p Valueï¼ˆ0.05ï¼‰ï¼Œä¼šå¾—åˆ°å¸å…ˆéªŒæ›´ä½ä¼°çš„äº‹åæ¦‚ç‡
![[Improving your statistical inferences_image_20.png]]
![[Improving your statistical inferences_image_21.png]]

# Multiple Comparisons, Statistical Power, Pre-Registration

## Type 1 error control

Iå‹é”™è¯¯ï¼Œè™šæ— å‡è®¾ä¸ºçœŸä½†æ˜¯ç»“æœæ˜¾è‘—çš„æƒ…å†µ

Iå‹é”™è¯¯å¢åŠ çš„ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯è¿›è¡Œå¤šé‡æ¯”è¾ƒï¼Œä»è€Œå¯¼è‡´çŠ¯ä¸‹ç»“è®ºæœ‰æ•ˆæœå®é™…å´æ²¡æœ‰æ•ˆæœçš„æ¬¡æ•°å¢åŠ ã€‚

In a 2x2x2 ANOVA, there are 7 tests. Type 1 error rate: 1-(0.95)$^7$=30%

æ¯”è¾ƒæ¬¡æ•°ä¸Iç±»é”™è¯¯ä¹‹é—´çš„å…³ç³»
![[Improving your statistical inferences_image_22.png]]
æ¯”è¾ƒå¸¸è§çš„æ ¡æ­£æ–¹å¼ï¼ˆBonferroni Correction, or Dunn Correctionï¼‰

$\frac{\alpha}{\text { number of tests }}$ æˆ–( $p \times$ number of tests)

å½“ä½ åªå…³æ³¨ä¸€ç§æ¯”è¾ƒï¼Œä¾‹å¦‚ä¹‹å‰çš„ 2x2x2 ANOVAï¼Œåªå…³æ³¨ 2x2x2çš„ç»“æœï¼Œé‚£ä¹ˆå¯ä»¥ä¸åšæ ¡æ­£ã€‚ä½†æ˜¯éœ€è¦åœ¨åˆ†æå‰è¡¨æ˜é¢„æµ‹ã€‚

Optional stopping: Collecting data until p < 0.05 inflates the Type 1 error.

åºåˆ—åˆ†æï¼ˆSequential analysisï¼‰å¯ä»¥æ§åˆ¶è¡¥å……ç ”ç©¶å¢åŠ Iç±»é”™è¯¯çš„é£é™©

![[Improving your statistical inferences_image_23.png]]
![[Improving your statistical inferences_image_24.png]]
## Type 2 error control

Iå‹é”™è¯¯ï¼Œè™šæ— å‡è®¾ä¸ºå‡ä½†æ˜¯ç»“æœæ˜¾è‘—çš„æƒ…å†µ

ä¸åŒæ•ˆåº”é‡å‘ç°æ˜¾è‘—æ€§ç»“æœçš„ç›¸å¯¹æ£€éªŒåŠ›
With n = 100, you had 95% power to observe a d=0.5

![[Improving your statistical inferences_image_25.png]]

![[Improving your statistical inferences_image_26.png]]

Iç±»é”™è¯¯ å¯ä»¥è¢«çº æ­£ï¼ŒIIç±»é”™è¯¯å¯èƒ½ä¼šæ›´ä¸¥é‡

## Pre-registration

é¢„æ³¨å†Œæ˜¯æ§åˆ¶Iç±»é”™è¯¯çš„è‰¯å¥½æ–¹å¼ã€‚

æŠ¥å‘Šä½œè€…è¦åœ¨å¼€å§‹æ”¶é›†èµ„æ–™å‰å†™å‡ºæŠ¥å‘Šï¼Œä¹‹åæŒ‰ç…§æŠ¥å‘Šåˆ†æèµ„æ–™ã€‚

æŠ¥å‘Šçš„é‡ç‚¹æ˜¯é™ˆåˆ—éªŒè¯ä¸»è¦å‡è®¾çš„æ‰€æœ‰åˆ†æè®¡åˆ’ç»†èŠ‚ã€‚

ä¹Ÿå¯ä»¥ç»§ç»­å®æ–½éªŒè¯æ€§åˆ†æï¼Œè¿™æ˜¯ä»‹äºé¢„æ³¨å†Œçš„éªŒè¯æ€§ç ”ç©¶ä¸æ¢ç´¢è¡Œç ”ç©¶ä¹‹é—´ã€‚

æ¢ç´¢æ€§åˆ†æä¼šå¢åŠ æœªçŸ¥çš„IIç±»é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•ï¼š
- äº‹å‰å®£å‘Šæ ·æœ¬é‡
- å¯¹äºæ¯ä¸€é¡¹æ£€éªŒå®šä¹‰è‡ªå˜é‡ä¸å› å˜é‡
- æè¿°åˆ†æè®¡åˆ’ï¼Œä¸ç»Ÿè®¡æ£€éªŒæ–¹æ³•
# Effect Sizes

## Effect Sizes

æ•ˆåº”é‡çš„æ„ä¹‰ï¼š
- ä½œè€…å¯ä»¥ä¼ é€’ç»“æœçš„å®é™…æ˜¾è‘—æ€§ practical significance
- æ•ˆåº”é‡å¯ä»¥å¸®åŠ©ä½œè€…å®ç°å…ƒåˆ†ææ–¹æ³•çš„ç»“è®º
- æ•ˆåº”é‡å¯ä»¥å…è®¸ç ”ç©¶è€…å»å‘ˆç°ç»Ÿè®¡æ£€éªŒåŠ›åˆ†æ

![[Improving your statistical inferences_image_27.png]]

æ•ˆåº”é‡åˆ†ä¸ºæ ‡å‡†åŒ–æ•ˆåº”é‡ä¸éæ ‡å‡†åŒ–æ•ˆåº”é‡ï¼Œéæ ‡å‡†åŒ–æ•ˆåº”é‡æ²¡æœ‰å•ä½ã€‚

ç»“æœæ˜¾è‘—ä½†æ˜¯æ•ˆåº”é‡å¾ˆå°ï¼Œå®é™…æ„ä¹‰ä¹Ÿä¼šå¾ˆå°ã€‚ä½†æ˜¯ä¹Ÿè¦å¼ºè°ƒå°æ•ˆåº”å¯¹å¤§ç¾¤ä½“çš„é‡è¦æ€§ã€‚
![[Improving your statistical inferences_image_28.png]]
æ•ˆåº”é‡çš„åˆ†ç±»ï¼š
- d family æ ‡å‡†åŒ–çš„å‡å€¼å·®å¼‚
- r family å˜é‡è§çš„å…³è”å¼ºåº¦

![[Improving your statistical inferences_image_29.png]]
## Cohen's d

Cohen's d is the difference divided by the standardÂ deviation

è¢«è¯•é—´dç›¸æ¯”è¢«è¯•å†…déœ€è¦è€ƒè™‘å˜é‡é—´çš„ç›¸å…³æ€§
![[Improving your statistical inferences_image_30.png]]

é€šè¿‡å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå¯ä»¥è®¡ç®—æ•ˆåº”é‡ã€‚

Cohen d æ˜¯ 0 åˆ°æ— ç©·çš„æ•°å€¼

Cohen dçš„å¯è§†åŒ–ï¼Œå³ä½¿æ˜¯å¤§æ•ˆåº”ï¼Œä¹Ÿæœ‰é‡å çš„éƒ¨åˆ†ã€‚

![[Improving your statistical inferences_image_31.png]]

Hedges' g is a unbiased version of Cohen's d

å°æ ·æœ¬é‡ä¸‹Cohen's dä¼šé«˜ä¼°çœŸå®æ•ˆæœ

$g=d \times\left(1-\frac{3}{4\left(n_1+n_2\right)-9}\right)$

## Correlation

ç›¸å…³æ˜¯-1åˆ°1çš„è¿ç»­æ•°æ®

é€šè¿‡æ¸¸æˆäº†è§£ç›¸å…³å¯è§†åŒ–çš„ç½‘ç«™[Guess the Correlation](https://www.guessthecorrelation.com/)

rä¸dçš„è½¬æ¢

$r=\frac{d_s}{\sqrt{d_S^2+\frac{N^2-2 N}{n_1 n_2}}}$

åœ¨æ–¹å·®åˆ†ææˆ–å›å½’åˆ†æä¸­ï¼Œé€šå¸¸ä¸ä¼šæŠ¥å‘Šrï¼Œè€Œæ˜¯æŠ¥å‘Š$R^2, \eta^2, \omega^2, \varepsilon^2$
è¿™äº›ä»£è¡¨æµ‹åˆ°çš„çœŸå®æ•ˆæœå˜å¼‚ï¼šProportion of total variance explained by an effect
$R^2, \eta^2$ æ˜¯çœŸå®æ•ˆåº”é‡çš„æœ‰åä¼°è®¡ï¼ˆå•å› ç´ æ–¹å·®åˆ†æä¸­$\eta^2 = \eta^2_{p}$ï¼‰
$\omega^2, \varepsilon^2$ æ˜¯çœŸå®æ•ˆåº”é‡çš„æ— åä¼°è®¡ï¼ˆæ˜¯æ¥è¿‘æ— åï¼‰

è¿™äº›éƒ½è¡¨ç¤ºå˜é‡xä¸yçš„å…³ç³»éœ€è¦å¤šé«˜ï¼Œæ‰èƒ½å‡é™¤æ•°æ®ä¸­çš„è¯¯å·®

![[Improving your statistical inferences_image_32.png]]
å·¦è¾¹ä»£è¡¨æ€»å¹³æ–¹å’Œ$S S_{tot}$ï¼Œå³è¾¹ä»£è¡¨æ®‹å·®å¹³æ–¹å’Œ$S S_{res}$
$R^2=\frac{S S_{\text {res }}}{S S_{\text {tot }}}$

Cohenâ€™s f å¯ä»¥é€šè¿‡$\eta^2$å¾—å‡º

Cohen (1988) has provided benchmarks to define small ( f = 0.10), medium (f = 0.25), and large (f = 0.50) effects.

To translate: Cohen (1988) has provided benchmarks to define small ( $\eta^2$= 0.0099), medium ( $\eta^2$= 0.0588), and large ( $\eta^2$= 0.1379) effects.

$f=\sqrt{\frac{\eta^2}{1-\eta^2}}$
![[Improving your statistical inferences_image_33.png]]

# Confidence Intervals, Sample Size Justification, p-Curve analysis

## Confidence Intervals

Confidence intervals are a statement about the percentage of confidence intervals that contain the true parameter value.

ç½®ä¿¡åŒºé—´æ˜¯ä¸€æ®µæ•°å€¼åŒºé—´ï¼Œå®£å‘ŠçœŸå®å‚æ•°å­˜åœ¨äºåŒºé—´ä¹‹å†…çš„ç™¾åˆ†æ¯”

å³ä¸€é¡¹ç ”ç©¶é‡å¤æ‰§è¡Œä¹‹åï¼Œè¿™äº›ç»“æœä¸­æœ‰95%åŒ…å«çœŸå®å‚æ•°å€¼

ä¸€é¡¹å‡å€¼ä¸º0çš„ç ”ç©¶ï¼Œ95%çš„ç½®ä¿¡åŒºé—´éƒ½åŒ…æ‹¬è¿™ä¸ªç»“æœï¼Œé»‘çº¿ä»£è¡¨çš„ä¸ºå¦å¤–5%

![[Improving your statistical inferences_image_34.png]]

æ•°æ®åˆ†æé€šå¸¸é€šè¿‡æ ·æœ¬Sampleé¢„æµ‹æ€»ä½“Populationï¼Œä¼šå­˜åœ¨ä¸ç¡®å®šæ€§

ä¾‹ï¼šä¸€ä¸ªè®¡ç®—å¥½çš„ç›¸å…³ç³»æ•°ï¼Œå…¶ä¸­çš„è“è‰²åŒºåŸŸä»£è¡¨ä¸ç¡®å®šæ€§ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼‰

![[Improving your statistical inferences_image_35.png]]
å¯¹äºæ­£æ€åˆ†å¸ƒï¼Œ95%ç½®ä¿¡åŒºé—´ä¸ºï¼š
$\mathrm{M} \pm 1.96 \times \mathrm{SE}$
Standard Error $(\mathrm{SE})=\mathrm{SD} / \sqrt{ } \mathrm{N}$

éšç€æ ·æœ¬é‡çš„å¢åŠ ï¼Œç½®ä¿¡åŒºé—´è¶ŠçŸ­
![[Improving your statistical inferences_image_36.png]]
ç½®ä¿¡åŒºé—´ä¸pç›´æ¥ç›¸å…³ï¼Œå¦‚æœ95%ç½®ä¿¡åŒºé—´åŒ…å«0ï¼Œé‚£ä¹ˆpä¸€å®š<.05

Highest density intervals (credible interval)
A 95% credible interval contains the values you find most plausible.
å¯ä¿¡åŒºé—´ï¼ˆè´å¶æ–¯ç»Ÿè®¡ä¸­ï¼‰ä»£è¡¨æœ‰95%çš„ä¿¡å¿ƒæ€»ä½“å‚æ•°è½åœ¨æ ·æœ¬å‚æ•°ä¸­

æœ‰æ—¶ï¼Œç ”ç©¶äººå‘˜å¸Œæœ›é¢„æµ‹å•ä¸ªå€¼æ‰€åœ¨çš„åŒºé—´ã€‚è¿™å°±æ˜¯æ‰€è°“çš„é¢„æµ‹åŒºé—´prediction intervalã€‚å®ƒæ€»æ˜¯æ¯”ç½®ä¿¡åŒºé—´å®½å¾—å¤šã€‚åŸå› æ˜¯å•ä¸ªè§‚æµ‹å€¼çš„å˜åŒ–å¯èƒ½å¾ˆå¤§ï¼Œä½†æœªæ¥æ ·æœ¬çš„å¹³å‡å€¼ï¼ˆ95% çš„æ—¶é—´éƒ½åœ¨æ­£å¸¸ç½®ä¿¡åŒºé—´å†…ï¼‰çš„å˜åŒ–ä¼šå°å¾—å¤šã€‚

## Sample Size Justification

å°æ ·æœ¬å­˜åœ¨è¾ƒå¤§çš„å˜å¼‚ï¼Œä»¥åŠæ›´å¤šçš„IIç±»é”™è¯¯ï¼Œå³æ›´å¤šçš„ä¸å‡†ç¡®è¯„ä¼°

![[Improving your statistical inferences_image_37.png]]

æ–¹æ³•1ï¼šæ ¹æ®ç½®ä¿¡åŒºé—´çš„å®½åº¦å†³å®šæ ·æœ¬é‡

![[Improving your statistical inferences_image_38.png]]

æ–¹æ³•2ï¼šæ ¹æ®æŒ‡å®šçš„ç»Ÿè®¡æ£€éªŒåŠ›

![[Improving your statistical inferences_image_39.png]]
ä½¿ç”¨ç»Ÿè®¡æ£€éªŒåŠ›æœ€å¥½ä½¿ç”¨éåæ•ˆåº”é‡(Hedges' g, $\varepsilon, \omega)$

å¦‚æœæ•ˆåº”é‡ä¸ç¡®å®šï¼Œä½¿ç”¨åºåˆ—åˆ†ææ˜¯ä¸€ç§ä¸é”™çš„æ–¹æ³•

è´å¶æ–¯ç»Ÿè®¡ä¸éœ€è¦å…³ç³»éœ€è¦æ”¶é›†çš„æ ·æœ¬æ•°ï¼Œæ”¶é›†åˆ°ä½ è®¤ä¸ºç»“æœé€‚å½“æ—¶å°±å¥½

## P Curve Analysis

P curve å¯ä»¥è¯„ä¼°æ–‡ä»¶æŠ½å±‰æ•ˆåº”ï¼Œå³ä½¿åˆ†æçš„åªæœ‰p<.05çš„å€¼

ç”¨äºå›ç­”çœŸå®æ•ˆæœä¸æ²¡æœ‰çœŸå®æ•ˆæœçš„på€¼åˆ†å¸ƒçœ‹èµ·æ¥ä¼šä¸ä¼šä¸ä¸€æ ·
ç»¿çº¿æ˜¯æœ‰æ•ˆæœçš„p curve è“çº¿æ˜¯æ— æ•ˆæœçš„ p curve
![[Improving your statistical inferences_image_40.png]]

å½“ p curve æ¥è¿‘æ— æ•ˆæ—¶ï¼Œæš—ç¤ºæ•°æ®æ— æ³•æ”¯æŒç ”ç©¶è€…çš„å‡è®¾

# Philosophy of Science & Theory

## Philosophy of Science

åšæœ‰æ•ˆæ¨è®ºçš„ä¸€ç§é€»è¾‘æ–¹æ³•ï¼šå¦å®šæ¡ä»¶ Modus Tollensï¼Œ å³å¯è¯ä¼ªæ€§
![[Improving your statistical inferences_image_41.png]]
![[Improving your statistical inferences_image_42.png]]

ç†è®ºçš„å†…æ ¸ä¸ç¼“å†²å¸¦ï¼Œç¼“å†²å¸¦å¯ä»¥éƒ¨åˆ†è°ƒæ•´ï¼Œä½†æ˜¯è¦ä¿ç•™å†…æ ¸
![[Improving your statistical inferences_image_43.png]]

## The Null is Always False

Cohen: é›¶å‡è®¾æ£€éªŒå­—é¢ä¸Šè¯´å°±æ˜¯ç°å®ä¸­ä¸å¯èƒ½å­˜åœ¨çš„äº‹

Crud Factor: ç ”ç©¶ä¸­çš„æ— å…³å› å­
åªè¦èµ„æ–™å¤Ÿå¤šï¼Œè¿™äº›å› ç´ éƒ½ä¼šå‘ˆç°æ˜¾è‘—ç»“æœ

Crud is systematic noise (there are true effects that you are not interested in), Type 1 errors are variable noise (no true effects).

å› æ­¤ï¼ŒéšæœºåŒ–åœ¨è¯æ˜æ˜¾è‘—æœ‰æ•ˆæ€§æœ‰å¾ˆé‡è¦çš„æ„ä¹‰

NHST rejects the null compared to any alternative. Point predictions are rare.

Strong Inference: Crucial experiments that exclude one alternative hypothesisã€‚å…³é”®å®éªŒé€šå¸¸å¯ä»¥æ’é™¤æœ€ä¸å¯èƒ½çš„å‡è®¾ã€‚

## Theory Construction

ç†è®ºä¸ç»Ÿè®¡å‡è®¾ä¸æ•°æ®çš„å…³ç³»
![[Improving your statistical inferences_image_44.png]]

å¦‚ä½•å»ºæ„ç†è®ºï¼š
- æ€æƒ³å®éªŒï¼šå¦‚æœæˆ‘è‡ªå·±åœ¨è¿™ä¸ªç†è®ºä¸­ï¼Œæˆ‘ä¼šæ€ä¹ˆåšï¼Œä¼šç¬¦åˆç†è®ºå—ï¼Ÿ
```
åè¿‡æ¥ä¹Ÿå¯ä»¥æˆ‘è¿™ä¹ˆåšäº†ï¼Œæ˜¯å› ä¸ºä»€ä¹ˆï¼Ÿå»å‘ç°ç†è®º
```
- è¡Œä¸ºé¢„æµ‹ï¼šå¯ä»¥ä»å‘¨è¾¹äººç‰©çš„è¡Œä¸ºå‘ç°å¯è¡Œçš„é¢„æµ‹
- ç†è§£ä»–äººè¡Œä¸ºèƒŒåçš„æƒ³æ³•
```
è®¿è°ˆå¯ä»¥å®ç°
```

å¯¹ç”Ÿæ´»ä¸­çš„ä¸€äº›äº‹ä»¶å‘ç° why å’Œ how
# Open Science 

## Replications

ä¸–ç•Œä¸Šæ²¡æœ‰ç™¾åˆ†ç™¾çš„å¤åˆ¶ï¼ˆå³éªŒè¯åˆè¯•å‘ç°çš„å¯é æ€§ï¼‰ï¼Œå› æ­¤ç›®çš„æˆäº†ç†è®ºå¤åˆ¶ï¼ˆéªŒè¯ç†è®ºçš„å¯é æ€§ï¼‰

ç›®æ ‡1ï¼šå®šä¹‰Iç±»é”™è¯¯
ç›®æ ‡2ï¼šæ§åˆ¶äººä¸ºå› ç´ 
ç›®æ ‡3ï¼šæ¨å¹¿åˆ°æ–°çš„äººå£ä¸­
ç›®æ ‡4ï¼šéªŒè¯æ½œåœ¨å‡è®¾

æ½œåœ¨çš„ä¸å¯å¤åˆ¶ç ”ç©¶ï¼šä½çš„ç»Ÿè®¡æ£€éªŒåŠ›ï¼Œé«˜çš„p valueï¼Œä»¤äººæƒŠè®¶çš„ç»“æœ
![[Improving your statistical inferences_image_45.png]]

## Publication Bias

å‡ºç‰ˆåå€šæœ€å¤§çš„åŸå› æ˜¯ç§‘å­¦ç•Œä»¥p = 0.05ä½œä¸ºæ ‡å‡†

è¿™ä¹Ÿå¯¼è‡´çœ‹åˆ°çš„æ–‡çŒ®å‡ ä¹éƒ½æ”¯æŒæŸç§å‡è®¾

![[Improving your statistical inferences_image_46.png]]
å¸¸è§çš„å‡ºç‰ˆåå€šæ ¡æ­£æ–¹æ³•ï¼š
- å‰ªè¡¥æ³•
- å¤±å®‰å…¨ç³»æ•°ï¼ˆä¸æ¨èç”¨ï¼Œå‘æ˜è€…æ‰¿è®¤å­˜åœ¨è®¡ç®—é—®é¢˜ï¼‰
- Egger å›é¡¾
- P curve
## Open Science

å¼€æ”¾ç§‘å­¦åŒ…æ‹¬ï¼šæ•°æ®ã€ææ–™ã€å‡ºç‰ˆ

æŸ¥è¯¢ä½ å¯ä»¥å¦‚ä½•åˆ†äº«ä½ å‘è¡¨çš„è®ºæ–‡
![[Improving your statistical inferences_image_47.png]]

æ•°æ®å…±äº«ï¼š

æœ€å¥½æä¾›åˆ†æè„šæœ¬ä¸åŸå§‹æ•°æ®ï¼Œæœ€å¥½åŠ ä¸Šæ³¨é‡Š

![[Improving your statistical inferences_image_48.png]]

å‚¨å­˜èµ„æ–™çš„äº‘ç«¯æœåŠ¡å™¨

![[Improving your statistical inferences_image_49.png]]

OSFå¹³å°

![[Improving your statistical inferences_image_50.png]]